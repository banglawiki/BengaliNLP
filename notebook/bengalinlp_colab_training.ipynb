{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bengalinlp_colab_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banglawiki/bengalinlp/blob/bengalinlp_4_dev/notebook/bengalinlp_colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BengaliNLP\n",
        "\n",
        "BengaliNLP is a natural language processing toolkit for Bengali Language. This tool will help you to tokenize Bengali text, Embedding Bengali words, Bengali POS Tagging, Construct Neural Model for Bengali NLP purposes.\n",
        "\n",
        "Here we are prodiving training approach of different model using **BengaliNLP**"
      ],
      "metadata": {
        "id": "0SQ0x9bh9QsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "MuT4uyIf5-Gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# !pip install -U bengalinlp_toolkit\n",
        "!pip install bengalinlp==1.0.0.dev1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bengalinlp==1.0.0.dev1\n",
            "  Downloading bengalinlp_toolkit-4.0.0.dev1-py3-none-any.whl (22 kB)\n",
            "Collecting sentencepiece (from bengalinlp==1.0.0.dev1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bengalinlp==1.0.0.dev1) (4.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bengalinlp==1.0.0.dev1) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bengalinlp==1.0.0.dev1) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bengalinlp==1.0.0.dev1) (1.10.1)\n",
            "Collecting sklearn-crfsuite (from bengalinlp==1.0.0.dev1)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bengalinlp==1.0.0.dev1) (4.66.0)\n",
            "Collecting ftfy (from bengalinlp==1.0.0.dev1)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji==1.7.0 (from bengalinlp==1.0.0.dev1)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bengalinlp==1.0.0.dev1) (2.31.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->bengalinlp==1.0.0.dev1) (0.2.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bengalinlp==1.0.0.dev1) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bengalinlp==1.0.0.dev1) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bengalinlp==1.0.0.dev1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bengalinlp==1.0.0.dev1) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp==1.0.0.dev1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp==1.0.0.dev1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp==1.0.0.dev1) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp==1.0.0.dev1) (2023.7.22)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->bengalinlp==1.0.0.dev1)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bengalinlp==1.0.0.dev1) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bengalinlp==1.0.0.dev1) (0.9.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=97760af148270953ae7148c41fafbe382f5d397c2613ef4f7e1534ff34cf8775\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: sentencepiece, python-crfsuite, emoji, sklearn-crfsuite, ftfy, bengalinlp\n",
            "Successfully installed bengalinlp-4.0.0.dev1 emoji-1.7.0 ftfy-6.1.1 python-crfsuite-0.9.9 sentencepiece-0.1.99 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "metadata": {
        "id": "KJN642aj5nVc",
        "outputId": "f51e370b-2244-4a67-a3d6-e63a0f2cb462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Download Bengali Wiki data"
      ],
      "metadata": {
        "id": "IWy0qUdy6BY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\"\n",
        "output = \"bn_wiki_data.txt.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "!unzip bn_wiki_data.txt.zip\n",
        "!rm -rf bn_wiki_data.txt.zip"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\n",
            "To: /content/bn_wiki_data.txt.zip\n",
            "100%|██████████| 69.2M/69.2M [00:00<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bn_wiki_data.txt.zip\n",
            "  inflating: bn_wiki_data.txt        \n"
          ]
        }
      ],
      "metadata": {
        "id": "AcwFE8le5yTF",
        "outputId": "73ed584d-b40f-4532-aa07-59cd48cac2c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Text Data"
      ],
      "metadata": {
        "id": "ywTQFAeU6QEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sample.txt\n",
        "চলতি সপ্তাহের শেষে সর্বজনীন পেনশন কর্মসূচির উদ্বোধন হতে যাচ্ছে। প্রধানমন্ত্রীর কার্যালয় থেকে অর্থ মন্ত্রণালয়ে পাঠানো চিঠিতে বলা হয়েছে, প্রধানমন্ত্রী শেখ হাসিনা আগামী ১৭ আগস্ট বৃহস্পতিবার ভার্চ্যুয়াল পদ্ধতিতে এ কর্মসূচির উদ্বোধন করবেন।\n",
        "\n",
        "ওই দিন সকাল ১০টায় ঢাকায় গণভবন থেকে প্রধানমন্ত্রী পেনশন কর্মসূচির উদ্বোধন করবেন। অনুষ্ঠানে সংযুক্ত থাকবে গোপালগঞ্জ, বাগেরহাট, রংপুর জেলা ও সৌদি আরবের বাংলাদেশ দূতাবাস।\n",
        "এর আগে অর্থ বিভাগ সূত্র প্রথম আলোকে জানিয়েছিল, সমাজের বিভিন্ন শ্রেণি-পেশার মানুষের কথা বিবেচনায় নিয়ে সর্বজনীন পেনশন কর্মসূচি চালু করা হচ্ছে। আপাতত চার শ্রেণির জনগোষ্ঠীর জন্য চার ধরনের পেনশন কর্মসূচি চালু করা হচ্ছে। এগুলোর নাম হচ্ছে প্রগতি, সুরক্ষা, সমতা ও প্রবাসী।\n",
        "\n",
        "\n",
        "এর মধ্যে বেসরকারি খাতের চাকরিজীবীদের জন্য ‘প্রগতি’, স্বকর্মে নিয়োজিত ব্যক্তিদের জন্য ‘সুরক্ষা’, প্রবাসী বাংলাদেশিদের জন্য ‘প্রবাসী’ ও দেশের নিম্ন আয়ের জনগোষ্ঠীর জন্য ‘সমতা’ শীর্ষক কর্মসূচি চালু করা হবে।\n",
        "\n",
        "সর্বজনীন পেনশন কর্মসূচি এমনভাবে করা হচ্ছে, যাতে দেশের ১৮ থেকে ৫০ বছর বয়সী সব নাগরিকই এ ব্যবস্থার আওতায় আসতে পারেন এবং ৬০ বছর বয়স থেকে তাঁরা আজীবন পেনশন পাবেন। শুরুর দিকে চিন্তা না থাকলেও পরে ৫০ বছরের বেশি বয়সীদেরও পেনশন কর্মসূচির আওতায় রাখার সুযোগ তৈরি করা হয়েছে। তাঁরা টানা ১০ বছর চাঁদা দেওয়ার পর পেনশন সুবিধা পাবেন।\n",
        "\n"
      ],
      "metadata": {
        "id": "sL96g8j16PWG",
        "outputId": "5c31e464-ac4e-4e07-aeae-369b981feb0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Here we present `bengali sentencepiece`, `bengali word2vec`, `bengali fasttext` training on `bengali wikipedia data`\n",
        "\n",
        "Training time will depend on data size."
      ],
      "metadata": {
        "id": "350KPo4D6Z4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Sentencepice Model\n",
        "\n",
        "After successfully compiling the below code will produce two file:\n",
        "\n",
        "* `wiki_sp.model`\n",
        "* `wiki_sp.vecab`"
      ],
      "metadata": {
        "id": "I_wHJFOW6dlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from bengalinlp import SentencepieceTrainer\n",
        "\n",
        "data = \"sample.txt\"\n",
        "vocab_size = 100\n",
        "model_prefix = \"model\"\n",
        "\n",
        "trainer = SentencepieceTrainer(\n",
        "   data=data,\n",
        "   vocab_size=vocab_size,\n",
        "   model_prefix=model_prefix\n",
        ")\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.model and model.vocab is saved on your current directory\n"
          ]
        }
      ],
      "metadata": {
        "id": "8l7DUWI66MD4",
        "outputId": "89201111-80d5-42d0-bdc8-d31ab4657229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Word2Vec Model\n",
        "\n",
        "After successfully compiling it will produce three file.\n",
        "\n",
        "* `wiki_word2vec.model`\n",
        "* `wiki_word2vec.vector`\n",
        "* `wiki_word2vec.model.trainables.syn1neg.npy`\n",
        "* `wiki_word2vec..model.wv.vectors.npy`\n"
      ],
      "metadata": {
        "id": "k-k4Dszo61v2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "from bengalinlp import Word2VecTraining\n",
        "\n",
        "trainer = Word2VecTraining()\n",
        "\n",
        "data_file = \"sample.txt\" # or you can pass custom sentence tokens as list of list\n",
        "model_name = \"test_model.model\"\n",
        "vector_name = \"test_vector.vector\"\n",
        "trainer.train(data_file, model_name, vector_name, epochs=5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started.......\n",
            "please wait.....it will take time according to your data size and computation capability\n",
            "train completed successfully\n",
            "trianing loss: 44.43490219116211\n",
            "model and vector saving...\n",
            "model and vector saved as test_model.model and test_vector.vector\n"
          ]
        }
      ],
      "metadata": {
        "id": "OphHV5Yp60KW",
        "outputId": "7af25b9a-b53d-4f09-9e8d-02e8e70b82da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-training or resume Bengali word2vec training"
      ],
      "metadata": {
        "id": "wdMRUO0jzV8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "from bengalinlp import Word2VecTraining\n",
        "\n",
        "trainer = Word2VecTraining()\n",
        "\n",
        "trained_model_path = \"test_model.model\"\n",
        "data_file = \"sample.txt\"\n",
        "model_name = \"test_model.model\"\n",
        "vector_name = \"test_vector.vector\"\n",
        "trainer.pretrain(trained_model_path, data_file, model_name, vector_name, epochs=5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loading ....\n",
            "vocab building with new sentences\n",
            "pre-training started.......\n",
            "please wait.....it will take time according to your data size and computation capability\n",
            "pre-train completed successfully\n",
            "pre-trianing loss: 0.0\n",
            "model and vector saving...\n",
            "model and vector saved as test_model.model and test_vector.vector\n"
          ]
        }
      ],
      "metadata": {
        "id": "i1aTn9cnzV8v",
        "outputId": "ab71f451-e873-4867-8a9c-f0800a034d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Fasttext Model\n",
        "First of all install `fasttext` using `pip install fasttext` and restart runtime.\n",
        "\n",
        "After successfully training it will produce:\n",
        "* `wiki_fasttext.bin`"
      ],
      "metadata": {
        "id": "TAMgr4WT8x2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "!pip install fasttext"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199770 sha256=c71df6599369022ad45445efe631c0f692a67fa2b1e78ca6ca3c0d04d4ca944c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ],
      "metadata": {
        "id": "JXptOhxg4s6r",
        "outputId": "8ef5c7cf-947a-49db-8518-378457cd55fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "from bengalinlp.embedding.fasttext import FasttextTrainer\n",
        "\n",
        "trainer = FasttextTrainer()\n",
        "\n",
        "data = \"sample.txt\"\n",
        "model_name = \"saved_model.bin\"\n",
        "epoch = 5\n",
        "trainer.train(data, model_name, epoch)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started.....\n",
            "training done! saving as saved_model.bin\n"
          ]
        }
      ],
      "metadata": {
        "id": "F67Yzdu08xBd",
        "outputId": "48c61612-9c72-4995-a5ce-3e3c7acfd01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bengali Doc2Vec"
      ],
      "metadata": {
        "id": "C_D6o84iz9le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bengalinlp import BengaliDoc2vecTrainer\n",
        "\n",
        "trainer = BengaliDoc2vecTrainer()\n",
        "\n",
        "text_files = \"./\"\n",
        "checkpoint_path = \"logs\"\n",
        "\n",
        "trainer.train(\n",
        "  text_files,\n",
        "  checkpoint_path=checkpoint_path,\n",
        "  vector_size=100,\n",
        "  min_count=2,\n",
        "  epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "5yNxSzNq0Al1",
        "outputId": "8ed3238e-fed4-44e8-e986-f8c344f4d48e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00, 454.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali POS TAGGING CRF model\n",
        "\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data:\n",
        "\n",
        "* `pos_model.pkl`"
      ],
      "metadata": {
        "id": "ZtsLVmOs9lgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "from bengalinlp import CRFTaggerTrainer\n",
        "\n",
        "trainer = CRFTaggerTrainer()\n",
        "\n",
        "model_name = \"pos_model.pkl\"\n",
        "train_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা',  'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "\n",
        "test_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা', 'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "\n",
        "trainer.train(model_name, train_data, test_data)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "Training Started........\n",
            "It will take time according to your dataset size...\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "1.0\n",
            "F1 Score(micro) is: \n",
            "1.0\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "VUKhbkaBE-CV",
        "outputId": "378b73d1-dc4d-4260-f0a4-39d9b2da0e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bengali NER model\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data:\n",
        "\n",
        "* `ner_model.pkl`"
      ],
      "metadata": {
        "id": "dPB7SBrKuSna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "from bengalinlp import CRFTaggerTrainer\n",
        "\n",
        "trainer = CRFTaggerTrainer()\n",
        "\n",
        "model_name = \"ner_model.pkl\"\n",
        "train_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "\n",
        "test_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "\n",
        "trainer.train(model_name, train_data, test_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "3\n",
            "Training Started........\n",
            "It will take time according to your dataset size...\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "1.0\n",
            "F1 Score(micro) is: \n",
            "1.0\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "of_1lkdW917n",
        "outputId": "f877b62a-b568-4d45-81ae-b4e770a95ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "qVrYxT5DulwP"
      }
    }
  ]
}
