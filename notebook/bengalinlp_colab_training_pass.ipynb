{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banglawiki/BengaliNLP/blob/main/notebook/bengalinlp_colab_training_pass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BengaliNLP\n",
        "\n",
        "BengaliNLP is a natural language processing toolkit for Bengali Language. This tool will help you to tokenize Bengali text, Embedding Bengali words, Bengali POS Tagging, Construct Neural Model for Bengali NLP purposes.\n",
        "\n",
        "Here we are prodiving training approach of different model using **BengaliNLP**"
      ],
      "metadata": {
        "id": "0SQ0x9bh9QsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "MuT4uyIf5-Gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install bengalinlp"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bengalinlp\n",
            "  Downloading bengalinlp-2.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sentencepiece==0.2.0 (from bengalinlp)\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting gensim==4.3.2 (from bengalinlp)\n",
            "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bengalinlp) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bengalinlp) (1.26.4)\n",
            "Collecting scipy==1.10.1 (from bengalinlp)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn-crfsuite==0.3.6 (from bengalinlp)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting tqdm==4.66.3 (from bengalinlp)\n",
            "  Downloading tqdm-4.66.3-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.2.0 (from bengalinlp)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting emoji==1.7.0 (from bengalinlp)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bengalinlp) (2.32.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.2.0->bengalinlp) (0.2.13)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2->bengalinlp) (7.0.4)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite==0.3.6->bengalinlp)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bengalinlp) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bengalinlp) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bengalinlp) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bengalinlp) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bengalinlp) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bengalinlp) (2024.8.30)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.2->bengalinlp) (1.16.0)\n",
            "Downloading bengalinlp-2.0.0-py3-none-any.whl (27 kB)\n",
            "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Downloading tqdm-4.66.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171031 sha256=72f4001f610ca983ecd6e8bc09b08b1b0e06b3cef6761c64199e1ea4f165b469\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: sentencepiece, python-crfsuite, emoji, tqdm, scipy, ftfy, sklearn-crfsuite, gensim, bengalinlp\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "Successfully installed bengalinlp-2.0.0 emoji-1.7.0 ftfy-6.2.0 gensim-4.3.2 python-crfsuite-0.9.10 scipy-1.10.1 sentencepiece-0.2.0 sklearn-crfsuite-0.3.6 tqdm-4.66.3\n"
          ]
        }
      ],
      "metadata": {
        "id": "KJN642aj5nVc",
        "outputId": "b172d133-bd1c-4ca1-ca4b-6ba519c8d01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Download Bengali Wiki data"
      ],
      "metadata": {
        "id": "IWy0qUdy6BY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Step 1: Download the file using gdown\n",
        "url = \"https://drive.google.com/uc?id=1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\"\n",
        "output = \"bn_wiki_data.txt.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Step 2: Unzip the file\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall()  # Extract all contents in the current directory\n",
        "\n",
        "# Step 3: Remove the zip file\n",
        "os.remove(output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\n",
            "From (redirected): https://drive.google.com/uc?id=1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM&confirm=t&uuid=dcfc98bd-c6c1-478b-bb61-3296e20b7e9c\n",
            "To: /content/bn_wiki_data.txt.zip\n",
            "100%|██████████| 69.2M/69.2M [00:04<00:00, 17.1MB/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "AcwFE8le5yTF",
        "outputId": "fc2596e8-ff1d-49f3-abf6-535b90578328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Text Data"
      ],
      "metadata": {
        "id": "ywTQFAeU6QEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sample.txt\n",
        "চলতি সপ্তাহের শেষে সর্বজনীন পেনশন কর্মসূচির উদ্বোধন হতে যাচ্ছে। প্রধানমন্ত্রীর কার্যালয় থেকে অর্থ মন্ত্রণালয়ে পাঠানো চিঠিতে বলা হয়েছে, প্রধানমন্ত্রী শেখ হাসিনা আগামী ১৭ আগস্ট বৃহস্পতিবার ভার্চ্যুয়াল পদ্ধতিতে এ কর্মসূচির উদ্বোধন করবেন।\n",
        "\n",
        "ওই দিন সকাল ১০টায় ঢাকায় গণভবন থেকে প্রধানমন্ত্রী পেনশন কর্মসূচির উদ্বোধন করবেন। অনুষ্ঠানে সংযুক্ত থাকবে গোপালগঞ্জ, বাগেরহাট, রংপুর জেলা ও সৌদি আরবের বাংলাদেশ দূতাবাস।\n",
        "এর আগে অর্থ বিভাগ সূত্র প্রথম আলোকে জানিয়েছিল, সমাজের বিভিন্ন শ্রেণি-পেশার মানুষের কথা বিবেচনায় নিয়ে সর্বজনীন পেনশন কর্মসূচি চালু করা হচ্ছে। আপাতত চার শ্রেণির জনগোষ্ঠীর জন্য চার ধরনের পেনশন কর্মসূচি চালু করা হচ্ছে। এগুলোর নাম হচ্ছে প্রগতি, সুরক্ষা, সমতা ও প্রবাসী।\n",
        "\n",
        "\n",
        "এর মধ্যে বেসরকারি খাতের চাকরিজীবীদের জন্য ‘প্রগতি’, স্বকর্মে নিয়োজিত ব্যক্তিদের জন্য ‘সুরক্ষা’, প্রবাসী বাংলাদেশিদের জন্য ‘প্রবাসী’ ও দেশের নিম্ন আয়ের জনগোষ্ঠীর জন্য ‘সমতা’ শীর্ষক কর্মসূচি চালু করা হবে।\n",
        "\n",
        "সর্বজনীন পেনশন কর্মসূচি এমনভাবে করা হচ্ছে, যাতে দেশের ১৮ থেকে ৫০ বছর বয়সী সব নাগরিকই এ ব্যবস্থার আওতায় আসতে পারেন এবং ৬০ বছর বয়স থেকে তাঁরা আজীবন পেনশন পাবেন। শুরুর দিকে চিন্তা না থাকলেও পরে ৫০ বছরের বেশি বয়সীদেরও পেনশন কর্মসূচির আওতায় রাখার সুযোগ তৈরি করা হয়েছে। তাঁরা টানা ১০ বছর চাঁদা দেওয়ার পর পেনশন সুবিধা পাবেন।\n",
        "\n"
      ],
      "metadata": {
        "id": "sL96g8j16PWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae72d1e-9bce-45c0-dece-2993c0a462b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Here we present `bengali sentencepiece`, `bengali word2vec`, `bengali fasttext` training on `bengali wikipedia data`\n",
        "\n",
        "Training time will depend on data size."
      ],
      "metadata": {
        "id": "350KPo4D6Z4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Sentencepice Model\n",
        "\n",
        "After successfully compiling the below code will produce two file:\n",
        "\n",
        "* `wiki_sp.model`\n",
        "* `wiki_sp.vecab`"
      ],
      "metadata": {
        "id": "I_wHJFOW6dlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "from bengalinlp import SentencepieceTrainer\n",
        "\n",
        "data = \"sample.txt\"\n",
        "vocab_size = 100\n",
        "model_prefix = \"model\"\n",
        "\n",
        "trainer = SentencepieceTrainer(\n",
        "   data=data,\n",
        "   vocab_size=vocab_size,\n",
        "   model_prefix=model_prefix\n",
        ")\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.model and model.vocab is saved on your current directory\n"
          ]
        }
      ],
      "metadata": {
        "id": "8l7DUWI66MD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2c7a91-4082-41fb-d444-0992b7dd67b4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Word2Vec Model\n",
        "\n",
        "After successfully compiling it will produce three file.\n",
        "\n",
        "* `wiki_word2vec.model`\n",
        "* `wiki_word2vec.vector`\n",
        "* `wiki_word2vec.model.trainables.syn1neg.npy`\n",
        "* `wiki_word2vec..model.wv.vectors.npy`\n"
      ],
      "metadata": {
        "id": "k-k4Dszo61v2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "from bengalinlp import Word2VecTraining\n",
        "\n",
        "trainer = Word2VecTraining()\n",
        "\n",
        "data_file = \"sample.txt\" # or you can pass custom sentence tokens as list of list\n",
        "model_name = \"test_model.model\"\n",
        "vector_name = \"test_vector.vector\"\n",
        "trainer.train(data_file, model_name, vector_name, epochs=5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started.......\n",
            "please wait.....it will take time according to your data size and computation capability\n",
            "train completed successfully\n",
            "trianing loss: 44.43490219116211\n",
            "model and vector saving...\n",
            "model and vector saved as test_model.model and test_vector.vector\n"
          ]
        }
      ],
      "metadata": {
        "id": "OphHV5Yp60KW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df0f2e1-ea7a-4db7-8b1d-b2f3235c9d34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-training or resume Bengali word2vec training"
      ],
      "metadata": {
        "id": "wdMRUO0jzV8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "from bengalinlp import Word2VecTraining\n",
        "\n",
        "trainer = Word2VecTraining()\n",
        "\n",
        "trained_model_path = \"test_model.model\"\n",
        "data_file = \"sample.txt\"\n",
        "model_name = \"test_model.model\"\n",
        "vector_name = \"test_vector.vector\"\n",
        "trainer.pretrain(trained_model_path, data_file, model_name, vector_name, epochs=5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loading ....\n",
            "vocab building with new sentences\n",
            "pre-training started.......\n",
            "please wait.....it will take time according to your data size and computation capability\n",
            "pre-train completed successfully\n",
            "pre-trianing loss: 0.0\n",
            "model and vector saving...\n",
            "model and vector saved as test_model.model and test_vector.vector\n"
          ]
        }
      ],
      "metadata": {
        "id": "i1aTn9cnzV8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd81513-9db4-4ffc-8d2c-5d9518b94e76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Fasttext Model\n",
        "First of all install `fasttext` using `pip install fasttext` and restart runtime.\n",
        "\n",
        "After successfully training it will produce:\n",
        "* `wiki_fasttext.bin`"
      ],
      "metadata": {
        "id": "TAMgr4WT8x2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "!pip install fasttext"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n"
          ]
        }
      ],
      "metadata": {
        "id": "JXptOhxg4s6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a9954b-60db-454c-d91d-d64b0c948204"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "from bengalinlp.embedding.fasttext import FasttextTrainer\n",
        "\n",
        "trainer = FasttextTrainer()\n",
        "\n",
        "data = \"sample.txt\"\n",
        "model_name = \"saved_model.bin\"\n",
        "epoch = 5\n",
        "trainer.train(data, model_name, epoch)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started.....\n",
            "training done! saving as saved_model.bin\n"
          ]
        }
      ],
      "metadata": {
        "id": "F67Yzdu08xBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d08975-31bc-4241-9983-6f9a13eed0e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bengali Doc2Vec"
      ],
      "metadata": {
        "id": "C_D6o84iz9le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bengalinlp import BengaliDoc2vecTrainer\n",
        "\n",
        "trainer = BengaliDoc2vecTrainer()\n",
        "\n",
        "text_files = \"./\"\n",
        "checkpoint_path = \"logs\"\n",
        "\n",
        "trainer.train(\n",
        "  text_files,\n",
        "  checkpoint_path=checkpoint_path,\n",
        "  vector_size=100,\n",
        "  min_count=2,\n",
        "  epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "5yNxSzNq0Al1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b217fa5-be46-479c-c5c0-d507485182d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [02:44, 82.22s/it] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali POS TAGGING CRF model\n",
        "\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data:\n",
        "\n",
        "* `pos_model.pkl`"
      ],
      "metadata": {
        "id": "ZtsLVmOs9lgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "from bengalinlp import CRFTaggerTrainer\n",
        "\n",
        "trainer = CRFTaggerTrainer()\n",
        "\n",
        "model_name = \"pos_model.pkl\"\n",
        "train_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা',  'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "\n",
        "test_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা', 'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "\n",
        "trainer.train(model_name, train_data, test_data)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "Training Started........\n",
            "It will take time according to your dataset size...\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "1.0\n",
            "F1 Score(micro) is: \n",
            "1.0\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "VUKhbkaBE-CV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de55e6a-6a01-4432-fb9b-8ded38374433"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FLtsySMSNkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bengali NER model\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data:\n",
        "\n",
        "* `ner_model.pkl`"
      ],
      "metadata": {
        "id": "dPB7SBrKuSna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "from bengalinlp import CRFTaggerTrainer\n",
        "\n",
        "trainer = CRFTaggerTrainer()\n",
        "\n",
        "model_name = \"ner_model.pkl\"\n",
        "train_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "\n",
        "test_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "\n",
        "trainer.train(model_name, train_data, test_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "3\n",
            "Training Started........\n",
            "It will take time according to your dataset size...\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "1.0\n",
            "F1 Score(micro) is: \n",
            "1.0\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "of_1lkdW917n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa991f9-b07d-490f-8006-d0a748c0f34e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "qVrYxT5DulwP"
      }
    }
  ]
}
